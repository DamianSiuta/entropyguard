# üõ°Ô∏è EntropyGuard - Kompleksowa Dokumentacja Projektu

## üìã Spis Tre≈õci

1. [Wprowadzenie i Cel Projektu](#wprowadzenie-i-cel-projektu)
2. [Problem: Model Collapse](#problem-model-collapse)
3. [Architektura Systemu](#architektura-systemu)
4. [Szczeg√≥≈Çowy Pipeline](#szczeg√≥≈Çowy-pipeline)
5. [Komponenty i Modu≈Çy](#komponenty-i-modu≈Çy)
6. [Hybrid Deduplication Engine](#hybrid-deduplication-engine)
7. [Memory Management i Optymalizacje](#memory-management-i-optymalizacje)
8. [CLI Interface](#cli-interface)
9. [Error Handling](#error-handling)
10. [Configuration Management](#configuration-management)
11. [Performance i Benchmarking](#performance-i-benchmarking)
12. [Testing Strategy](#testing-strategy)
13. [Deployment i Distribution](#deployment-i-distribution)
14. [Roadmap i Wersjonowanie](#roadmap-i-wersjonowanie)

---

## 1. Wprowadzenie i Cel Projektu

### 1.1 Co to jest EntropyGuard?

**EntropyGuard** to zaawansowany, produkcyjny system in≈ºynierii danych zaprojektowany specjalnie do optymalizacji danych treningowych dla Large Language Models (LLM). Jest to kompleksowe narzƒôdzie CLI, kt√≥re rozwiƒÖzuje krytyczny problem degradacji jako≈õci modeli ML spowodowany treningiem na niskiej jako≈õci, zduplikowanych lub zanieczyszczonych danych.

### 1.2 G≈Ç√≥wne Cele Projektu

1. **Zapobieganie Model Collapse**: Eliminacja zduplikowanych i niskiej jako≈õci danych przed treningiem modeli
2. **Lokalne Przetwarzanie**: 100% lokalne wykonanie, bez wysy≈Çania danych do zewnƒôtrznych API
3. **Wysoka Wydajno≈õƒá**: Optymalizacja pod kƒÖtem szybko≈õci i efektywno≈õci pamiƒôciowej
4. **Enterprise-Grade**: Produkcyjna jako≈õƒá kodu z pe≈Çnym testowaniem i dokumentacjƒÖ
5. **Uniwersalno≈õƒá**: Obs≈Çuga wielu format√≥w danych i integracja z istniejƒÖcymi pipeline'ami

### 1.3 Kluczowe Warto≈õci

- **Privacy-First**: Wszystkie dane przetwarzane lokalnie, zero wyciek√≥w do chmury
- **Air-Gap Compatible**: Dzia≈Ça w ≈õrodowiskach izolowanych, bez dostƒôpu do internetu
- **CPU-Only**: Nie wymaga GPU, dzia≈Ça na standardowych serwerach
- **Open Source Core**: Podstawowa funkcjonalno≈õƒá dostƒôpna jako open source (MIT License)

---

## 2. Problem: Model Collapse

### 2.1 Czym jest Model Collapse?

**Model Collapse** to zjawisko, w kt√≥rym modele ML stopniowo tracƒÖ jako≈õƒá podczas treningu na danych zawierajƒÖcych:
- **Duplikaty**: Identyczne lub bardzo podobne przyk≈Çady
- **NiskƒÖ jako≈õƒá**: Puste, zanieczyszczone lub nieprawid≈Çowe dane
- **Bias**: Przesuniƒôcie w kierunku najczƒô≈õciej wystƒôpujƒÖcych wzorc√≥w

### 2.2 Dlaczego to Problem?

1. **Degradacja Jako≈õci**: Model uczy siƒô zamiast rzeczywistych wzorc√≥w, powtarza zduplikowane dane
2. **Waste of Resources**: Przetwarzanie zduplikowanych danych marnuje czas i zasoby
3. **Koszty**: W przypadku API (np. OpenAI embeddings), duplikaty generujƒÖ niepotrzebne koszty
4. **Compliance**: Zduplikowane dane mogƒÖ naruszaƒá zasady GDPR/CCPA

### 2.3 Jak EntropyGuard to RozwiƒÖzuje?

EntropyGuard implementuje **hybrid deduplication strategy**:
- **Stage 1**: Szybkie usuwanie dok≈Çadnych duplikat√≥w (hash-based)
- **Stage 2**: Wykrywanie semantycznych duplikat√≥w (AI-based)
- **Stage 3**: Walidacja i filtrowanie niskiej jako≈õci danych

---

## 3. Architektura Systemu

### 3.1 Og√≥lna Architektura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CLI Interface (main.py)                  ‚îÇ
‚îÇ  - Argument parsing                                         ‚îÇ
‚îÇ  - Signal handling (SIGINT/SIGTERM)                        ‚îÇ
‚îÇ  - Error handling & reporting                              ‚îÇ
‚îÇ  - Configuration loading                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Core Pipeline (pipeline.py)               ‚îÇ
‚îÇ  - Orchestration logic                                      ‚îÇ
‚îÇ  - Stage coordination                                       ‚îÇ
‚îÇ  - Memory profiling                                         ‚îÇ
‚îÇ  - Progress tracking                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ
       ‚ñº          ‚ñº          ‚ñº          ‚ñº          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇIngestion ‚îÇ ‚îÇSanitize  ‚îÇ ‚îÇChunking  ‚îÇ ‚îÇDedupe    ‚îÇ ‚îÇValidation‚îÇ
‚îÇ          ‚îÇ ‚îÇ          ‚îÇ ‚îÇ          ‚îÇ ‚îÇ          ‚îÇ ‚îÇ          ‚îÇ
‚îÇloader.py ‚îÇ ‚îÇsanitize  ‚îÇ ‚îÇsplitter  ‚îÇ ‚îÇembedder  ‚îÇ ‚îÇvalidator ‚îÇ
‚îÇ          ‚îÇ ‚îÇ_lazy.py  ‚îÇ ‚îÇ.py       ‚îÇ ‚îÇ+index.py ‚îÇ ‚îÇ.py       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 Struktura Modu≈Ç√≥w

```
src/entropyguard/
‚îú‚îÄ‚îÄ __init__.py              # Package initialization, version
‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îî‚îÄ‚îÄ main.py              # CLI entry point, argument parsing
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py          # Core exports
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py          # Main pipeline orchestration
‚îÇ   ‚îú‚îÄ‚îÄ errors.py            # Exception hierarchy
‚îÇ   ‚îú‚îÄ‚îÄ types.py             # Type definitions (TypedDict, dataclass)
‚îÇ   ‚îú‚îÄ‚îÄ config_loader.py     # Config file loading (JSON/YAML/TOML)
‚îÇ   ‚îú‚îÄ‚îÄ memory_profiler.py   # Memory usage tracking
‚îÇ   ‚îî‚îÄ‚îÄ sanitization_lazy.py # Lazy sanitization functions
‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îî‚îÄ‚îÄ loader.py            # Universal data loader (Excel/Parquet/CSV/JSONL)
‚îú‚îÄ‚îÄ sanitization/
‚îÇ   ‚îî‚îÄ‚îÄ core.py              # PII removal, HTML stripping
‚îú‚îÄ‚îÄ chunking/
‚îÇ   ‚îî‚îÄ‚îÄ splitter.py          # Text chunking for RAG
‚îú‚îÄ‚îÄ deduplication/
‚îÇ   ‚îú‚îÄ‚îÄ embedder.py          # Sentence-transformers wrapper
‚îÇ   ‚îî‚îÄ‚îÄ index.py             # FAISS vector index wrapper
‚îî‚îÄ‚îÄ validation/
    ‚îî‚îÄ‚îÄ validator.py         # Data quality validation
```

### 3.3 Design Principles

1. **Separation of Concerns**: Business logic (`core/`) oddzielona od CLI (`cli/`)
2. **Lazy Evaluation**: Polars LazyFrame dla efektywno≈õci pamiƒôciowej
3. **Type Safety**: Pe≈Çne type hints, TypedDict, dataclasses
4. **Structured Errors**: Hierarchia wyjƒÖtk√≥w z kodami b≈Çƒôd√≥w
5. **Testability**: Ka≈ºdy modu≈Ç testowalny niezale≈ºnie

---

## 4. Szczeg√≥≈Çowy Pipeline

### 4.1 Przep≈Çyw Danych

```
[Raw Data Input]
    ‚îÇ
    ‚îú‚îÄ JSONL/NDJSON
    ‚îú‚îÄ CSV
    ‚îú‚îÄ Excel (.xlsx)
    ‚îú‚îÄ Parquet
    ‚îî‚îÄ stdin (Unix pipes)
    ‚îÇ
    ‚ñº
[STEP 1: Ingestion]
    ‚îÇ
    ‚îú‚îÄ Auto-detect format
    ‚îú‚îÄ Load as Polars LazyFrame
    ‚îî‚îÄ Schema detection (lazy)
    ‚îÇ
    ‚ñº
[STEP 2: Schema Validation]
    ‚îÇ
    ‚îú‚îÄ Check required columns (if specified)
    ‚îú‚îÄ Auto-detect text column (if not specified)
    ‚îî‚îÄ All operations on LazyFrame (no materialization)
    ‚îÇ
    ‚ñº
[STEP 3: Sanitization]
    ‚îÇ
    ‚îú‚îÄ Lowercase normalization
    ‚îú‚îÄ Whitespace normalization
    ‚îú‚îÄ Drop nulls
    ‚îî‚îÄ PII removal (hybrid: lazy + chunked materialization)
    ‚îÇ
    ‚ñº
[STEP 4: Chunking (Optional)]
    ‚îÇ
    ‚îú‚îÄ Only if --chunk-size provided
    ‚îú‚îÄ Recursive splitting (paragraph ‚Üí line ‚Üí word ‚Üí char)
    ‚îú‚îÄ Configurable separators
    ‚îî‚îÄ Overlap support for RAG
    ‚îÇ
    ‚ñº
[STEP 5: Materialization]
    ‚îÇ
    ‚îú‚îÄ Collect LazyFrame to DataFrame
    ‚îú‚îÄ Add _original_index column
    ‚îî‚îÄ Prepare for deduplication
    ‚îÇ
    ‚ñº
[STEP 6: Stage 1 - Exact Deduplication]
    ‚îÇ
    ‚îú‚îÄ Calculate xxhash for each text
    ‚îú‚îÄ Group by hash
    ‚îú‚îÄ Keep first occurrence
    ‚îî‚îÄ Fast hash-based filtering
    ‚îÇ
    ‚ñº
[STEP 7: Stage 2 - Semantic Deduplication]
    ‚îÇ
    ‚îú‚îÄ Process in batches (configurable --batch-size)
    ‚îú‚îÄ Generate embeddings (sentence-transformers)
    ‚îú‚îÄ Add to FAISS index incrementally
    ‚îú‚îÄ Find duplicates (cosine similarity)
    ‚îî‚îÄ Filter semantic duplicates
    ‚îÇ
    ‚ñº
[STEP 8: Validation]
    ‚îÇ
    ‚îú‚îÄ Check min_length
    ‚îú‚îÄ Drop empty/null texts
    ‚îî‚îÄ Final quality gates
    ‚îÇ
    ‚ñº
[STEP 9: Output]
    ‚îÇ
    ‚îú‚îÄ Write to file (NDJSON)
    ‚îú‚îÄ Or stdout (for pipes)
    ‚îî‚îÄ Generate audit log (optional)
```

### 4.2 Szczeg√≥≈Çy Ka≈ºdego Etapu

#### STEP 1: Ingestion (`ingestion/loader.py`)

**Cel**: Za≈Çadowanie danych z r√≥≈ºnych format√≥w do jednolitej reprezentacji Polars LazyFrame.

**Obs≈Çugiwane formaty**:
- **NDJSON/JSONL**: `pl.scan_ndjson()` - natywne wsparcie Polars
- **CSV**: `pl.scan_csv()` - z auto-detection separator√≥w
- **Parquet**: `pl.scan_parquet()` - efektywny format kolumnowy
- **Excel**: `pl.read_excel()` ‚Üí `.lazy()` - wymaga `fastexcel`
- **JSON**: `pl.read_json()` ‚Üí `.lazy()` - dla pojedynczych plik√≥w JSON

**Dlaczego LazyFrame?**
- **Memory Efficiency**: Nie ≈Çaduje ca≈Çego datasetu do pamiƒôci
- **Query Optimization**: Polars optymalizuje zapytania przed wykonaniem
- **Streaming**: Mo≈ºliwo≈õƒá przetwarzania danych wiƒôkszych ni≈º RAM

**Implementacja**:
```python
def load_dataset(input_path: str) -> pl.LazyFrame:
    """Universal loader with format auto-detection."""
    path = Path(input_path)
    suffix = path.suffix.lower()
    
    if suffix in ['.ndjson', '.jsonl']:
        return pl.scan_ndjson(input_path)
    elif suffix == '.csv':
        return pl.scan_csv(input_path)
    elif suffix == '.parquet':
        return pl.scan_parquet(input_path)
    # ... etc
```

#### STEP 2: Schema Validation

**Cel**: Weryfikacja struktury danych bez materializacji.

**Operacje**:
- Sprawdzenie wymaganych kolumn (je≈õli `--required-columns` podane)
- Auto-detection kolumny tekstowej (pierwsza kolumna typu `Utf8`)
- Wszystko na `lf.schema` (tylko metadane, zero materializacji)

**Dlaczego Lazy?**
- Szybko≈õƒá: Sprawdzenie schematu to O(1) operacja
- Memory: Nie ≈Çadujemy danych do pamiƒôci
- Early Validation: B≈Çƒôdy wykrywane przed kosztownym przetwarzaniem

#### STEP 3: Sanitization (`core/sanitization_lazy.py`)

**Cel**: Normalizacja i czyszczenie tekstu.

**Operacje Lazy (na LazyFrame)**:
- `str.to_lowercase()` - lowercase
- `str.strip_chars()` - usuwanie bia≈Çych znak√≥w
- `drop_nulls()` - usuwanie nulli

**Operacje WymagajƒÖce Materializacji**:
- **PII Removal**: Wymaga Python functions (regex), wiƒôc materializujemy w chunkach
- **Hybrid Approach**: Lazy operations ‚Üí chunked materialization ‚Üí lazy again

**Dlaczego Hybrid?**
- PII removal jest wczesnym etapem (przed kosztownym embedding)
- G≈Ç√≥wny OOM risk jest w Stage 2 (embeddings)
- Chunked materialization pozwala przetwarzaƒá du≈ºe pliki bez OOM

**Implementacja**:
```python
def sanitize_lazyframe(lf: pl.LazyFrame, config, text_columns):
    # Lazy operations first
    lf = lf.with_columns([
        pl.col(col).str.to_lowercase().str.strip_chars()
        for col in text_columns
    ])
    lf = lf.drop_nulls()
    
    # PII removal requires materialization (chunked)
    if config.remove_pii:
        df = lf.collect()  # Materialize
        # Apply PII removal in chunks
        # ...
        lf = df.lazy()  # Back to lazy
    
    return lf
```

#### STEP 4: Chunking (`chunking/splitter.py`)

**Cel**: Podzia≈Ç d≈Çugich tekst√≥w na mniejsze fragmenty (dla RAG).

**Kiedy u≈ºywane?**
- Tylko je≈õli `--chunk-size` jest podane
- Typowo dla RAG workflows (Retrieval-Augmented Generation)

**Strategia**:
1. **Recursive Splitting**: Pr√≥buje podzieliƒá w kolejno≈õci:
   - Paragraph breaks (`\n\n`)
   - Line breaks (`\n`)
   - Word boundaries (` `)
   - Character-level (dla CJK languages)

2. **Overlap**: Konfigurowalny overlap miƒôdzy chunkami (domy≈õlnie 50 znak√≥w)

3. **Separators**: Mo≈ºliwo≈õƒá podania w≈Çasnych separator√≥w

**Dlaczego Materializacja?**
- Chunking wymaga Python functions (regex, string manipulation)
- Ale dzieje siƒô PRZED kosztownym embedding stage
- Akceptowalne trade-off

#### STEP 5: Materialization

**Cel**: Konwersja LazyFrame do DataFrame dla deduplication.

**Dlaczego Teraz?**
- Hash calculation (Stage 1) wymaga dostƒôpu do warto≈õci
- Polars hash operations sƒÖ szybkie
- G≈Ç√≥wny OOM risk jest w Stage 2 (embeddings), nie tutaj

**Operacje**:
- `lf.collect()` - materializacja
- Dodanie `_original_index` kolumny (dla tracking)
- Sprawdzenie czy dataset nie jest pusty

#### STEP 6: Stage 1 - Exact Deduplication

**Cel**: Szybkie usuniƒôcie dok≈Çadnych duplikat√≥w.

**Algorytm**:
1. Dla ka≈ºdego tekstu: oblicz `xxhash` (lub MD5 fallback)
2. Normalizacja przed hashowaniem: lowercase + whitespace normalization
3. Grupowanie po hash: `rank().over("_text_hash")`
4. Filtrowanie: `filter(rank == 1)` - zostaw tylko pierwszy

**Dlaczego xxhash?**
- **Szybko≈õƒá**: xxhash jest ~10x szybszy ni≈º MD5
- **Non-cryptographic**: Nie potrzebujemy kryptograficznego hash (tylko deduplication)
- **Deterministic**: Ten sam tekst ‚Üí ten sam hash

**Performance**:
- ~5000-6000 rows/second (na CPU)
- Memory: O(n) gdzie n = liczba unikalnych hash√≥w

**Implementacja**:
```python
# Calculate hashes
text_hashes = [calculate_text_hash(text) for text in texts]

# Add hash column
df = df.with_columns(pl.Series("_text_hash", text_hashes))

# Group by hash, keep first
lf_dedup = df.lazy().with_columns(
    pl.col("_original_index").rank("dense").over("_text_hash").alias("_hash_rank")
).filter(pl.col("_hash_rank") == 1)
```

#### STEP 7: Stage 2 - Semantic Deduplication

**Cel**: Wykrywanie semantycznych duplikat√≥w (r√≥≈ºne s≈Çowa, podobne znaczenie).

**Algorytm**:
1. **Batched Embeddings**: 
   - Podziel teksty na batche (domy≈õlnie 10,000)
   - Dla ka≈ºdego batcha: wygeneruj embeddings (sentence-transformers)
   - Dodaj do FAISS index incrementally

2. **FAISS Index**:
   - Typ: `IndexFlatL2` (L2 distance)
   - Dimension: 384 (dla `all-MiniLM-L6-v2`)
   - Keep index in memory (nie release chunks)

3. **Duplicate Detection**:
   - Po dodaniu wszystkich embeddings: `index.find_duplicates(threshold)`
   - Threshold: `(2.0 * (1.0 - similarity)) ** 0.5` (konwersja cosine ‚Üí L2)
   - Dla ka≈ºdej grupy duplikat√≥w: zostaw pierwszy, usu≈Ñ resztƒô

**Dlaczego Batched?**
- **Memory Safety**: Nie ≈Çadujemy wszystkich embeddings naraz
- **Scalability**: Mo≈ºemy przetwarzaƒá miliony wierszy
- **Performance**: Batch processing jest efektywny dla GPU/CPU

**Dlaczego FAISS?**
- **Speed**: FAISS jest zoptymalizowany do vector search
- **Memory**: Kompaktowa reprezentacja wektor√≥w
- **Scalability**: Obs≈Çuguje miliony wektor√≥w

**Model Selection**:
- **Default**: `all-MiniLM-L6-v2` (384 dim, 22M params, fast)
- **Multilingual**: `paraphrase-multilingual-MiniLM-L12-v2` (384 dim, slower)
- **Configurable**: `--model-name` flag

**Implementacja**:
```python
# Initialize FAISS index
index = VectorIndex(dimension=384)

# Process in batches
for batch_idx in range(0, len(texts), batch_size):
    batch_texts = texts[batch_idx:batch_idx + batch_size]
    
    # Generate embeddings
    batch_embeddings = embedder.embed(batch_texts)
    
    # Add to index
    index.add_vectors(batch_embeddings)

# Find duplicates
duplicate_groups = index.find_duplicates(threshold=distance_threshold)
```

#### STEP 8: Validation

**Cel**: Finalne quality gates przed zapisem.

**Sprawdzane**:
- `min_length`: Minimalna d≈Çugo≈õƒá tekstu (domy≈õlnie 50 znak√≥w)
- Empty/null texts: Usuwanie pustych warto≈õci
- Stripped length: Po usuniƒôciu bia≈Çych znak√≥w

**Dlaczego Ostatnie?**
- Sprawdzamy po wszystkich transformacjach
- Zapewniamy ≈ºe output jest wysokiej jako≈õci

#### STEP 9: Output

**Cel**: Zapis przetworzonych danych.

**Formaty**:
- **NDJSON/JSONL**: Jeden JSON object per line
- **stdout**: Dla Unix pipes (wszystkie logi na stderr)

**Audit Log** (opcjonalny):
- JSON file z ka≈ºdym usuniƒôtym wierszem
- Pow√≥d usuniƒôcia (exact_duplicate, semantic_duplicate, validation)
- Original index dla traceability

---

## 5. Komponenty i Modu≈Çy

### 5.1 Core Module (`core/`)

#### `pipeline.py` - Main Orchestration

**Klasa**: `Pipeline`

**Odpowiedzialno≈õƒá**:
- Koordynacja wszystkich etap√≥w pipeline
- ZarzƒÖdzanie komponentami (validator, chunker, embedder, index)
- Error handling z structured exceptions
- Progress tracking (tqdm)
- Memory profiling integration

**Kluczowe Metody**:
- `__init__(model_name)`: Inicjalizacja komponent√≥w
- `run(config: PipelineConfig) -> PipelineResult`: G≈Ç√≥wna metoda wykonania

**Design Decisions**:
- **Separation**: Business logic oddzielona od CLI
- **Config Object**: Typed `PipelineConfig` dataclass zamiast dict
- **Result Object**: Typed `PipelineResult` TypedDict dla type safety
- **Exception Hierarchy**: Structured errors zamiast generic exceptions

#### `errors.py` - Exception Hierarchy

**Struktura**:
```python
PipelineError (base)
‚îú‚îÄ‚îÄ ValidationError (code=2)    # Input validation issues
‚îú‚îÄ‚îÄ ResourceError (code=3)      # OOM, disk space, IO
‚îî‚îÄ‚îÄ ProcessingError (code=1)    # Embedding, FAISS failures
```

**Dlaczego Hierarchia?**
- **Structured Handling**: CLI mo≈ºe obs≈Çu≈ºyƒá r√≥≈ºne typy b≈Çƒôd√≥w inaczej
- **Exit Codes**: R√≥≈ºne kody wyj≈õcia dla r√≥≈ºnych b≈Çƒôd√≥w
- **User-Friendly**: Konkretne komunikaty b≈Çƒôd√≥w z hints

**Atrybuty**:
- `message`: G≈Ç√≥wny komunikat b≈Çƒôdu
- `code`: Exit code (1, 2, 3)
- `category`: Kategoria b≈Çƒôdu (validation, resource, processing)
- `hint`: Opcjonalna wskaz√≥wka dla u≈ºytkownika

#### `types.py` - Type Definitions

**Typy**:
- `PipelineStats` (TypedDict): Statystyki pipeline (rows, duplicates, etc.)
- `PipelineResult` (TypedDict): Wynik pipeline (success, stats, output_path)
- `PipelineConfig` (dataclass): Konfiguracja pipeline

**Dlaczego TypedDict + dataclass?**
- **Type Safety**: MyPy mo≈ºe sprawdziƒá typy
- **Runtime Validation**: Pydantic mo≈ºe walidowaƒá (future)
- **IDE Support**: Autocomplete i type hints

#### `config_loader.py` - Configuration Management

**Funkcje**:
- `load_config_file(path)`: ≈Åadowanie z JSON/YAML/TOML
- `merge_config_with_args(config, args)`: Merge config file + CLI args

**Auto-Detection**:
- Szuka `.entropyguardrc.json/yaml/toml` w:
  - Current directory
  - Home directory

**Priority**:
1. CLI arguments (najwy≈ºszy priorytet)
2. Config file
3. Defaults

**Dlaczego Config Files?**
- **Convenience**: Nie trzeba powtarzaƒá argument√≥w
- **Team Consistency**: Wsp√≥lna konfiguracja w repo
- **CI/CD**: ≈Åatwiejsza automatyzacja

#### `memory_profiler.py` - Memory Tracking

**Klasa**: `MemoryProfiler`

**Funkcje**:
- `snapshot(stage)`: Wykonanie snapshot pamiƒôci na etapie
- `get_report() -> dict`: Generowanie raportu
- `save_report_json(path)`: Zapis do JSON
- `print_summary()`: Wy≈õwietlenie podsumowania

**Backend Options**:
- **psutil** (preferred): Dok≈Çadne monitorowanie procesu
- **tracemalloc** (fallback): Built-in Python, mniej dok≈Çadny

**U≈ºycie**:
- `--profile-memory`: W≈ÇƒÖcza profiling
- `--memory-report-path`: ≈öcie≈ºka do zapisu raportu

**Dlaczego Memory Profiling?**
- **Debugging OOM**: Identyfikacja etap√≥w z wysokim u≈ºyciem pamiƒôci
- **Optimization**: Znajdowanie bottleneck√≥w pamiƒôciowych
- **Capacity Planning**: Planowanie zasob√≥w dla du≈ºych dataset√≥w

#### `sanitization_lazy.py` - Lazy Sanitization

**Funkcja**: `sanitize_lazyframe(lf, config, text_columns) -> LazyFrame`

**Operacje**:
- Lazy: lowercase, strip, drop_nulls
- Hybrid: PII removal (chunked materialization)

**Dlaczego Lazy?**
- **Memory Efficiency**: Nie materializujemy ca≈Çego datasetu
- **Performance**: Polars optymalizuje operacje
- **Scalability**: Mo≈ºemy przetwarzaƒá dane > RAM

### 5.2 Ingestion Module (`ingestion/`)

#### `loader.py` - Universal Data Loader

**Funkcja**: `load_dataset(input_path) -> LazyFrame`

**Obs≈Çugiwane Formaty**:
- **NDJSON/JSONL**: `pl.scan_ndjson()` - streaming support
- **CSV**: `pl.scan_csv()` - auto-detection separator√≥w
- **Parquet**: `pl.scan_parquet()` - columnar format
- **Excel**: `pl.read_excel()` ‚Üí `.lazy()` - wymaga fastexcel
- **JSON**: `pl.read_json()` ‚Üí `.lazy()` - single JSON file

**Auto-Detection**:
- Na podstawie file extension
- Fallback: pr√≥ba wszystkich format√≥w

**Error Handling**:
- `FileNotFoundError`: Plik nie istnieje
- `ValueError`: Nieobs≈Çugiwany format lub corrupted file

### 5.3 Sanitization Module (`sanitization/`)

#### `core.py` - PII Removal & Text Cleaning

**Funkcje**:
- `remove_pii(text)`: Usuwanie emails, phones, IDs (regex-based)
- `remove_html_tags(text)`: Usuwanie HTML tags
- `normalize_text(text)`: Normalizacja whitespace

**PII Patterns**:
- Email: `[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}`
- Phone: R√≥≈ºne formaty (US, EU, etc.)
- SSN/ID: Konfigurowalne wzorce

**Dlaczego Regex?**
- **Speed**: Regex jest szybki dla prostych wzorc√≥w
- **No Dependencies**: Nie wymaga zewnƒôtrznych bibliotek
- **Configurable**: ≈Åatwo dodaƒá nowe wzorce

### 5.4 Chunking Module (`chunking/`)

#### `splitter.py` - Text Chunking

**Klasa**: `Chunker`

**Strategia**:
1. **Recursive Splitting**: Pr√≥buje podzieliƒá w kolejno≈õci separator√≥w
2. **Overlap**: Konfigurowalny overlap miƒôdzy chunkami
3. **Fallback**: Character-level dla CJK languages

**Separators Hierarchy**:
- Default: `["\n\n", "\n", " ", ""]` (paragraph ‚Üí line ‚Üí word ‚Üí char)
- Custom: `--separators` flag

**U≈ºycie**:
- RAG workflows (Retrieval-Augmented Generation)
- Long document processing
- Context window limitations

### 5.5 Deduplication Module (`deduplication/`)

#### `embedder.py` - Sentence Embeddings

**Klasa**: `Embedder`

**Backend**: `sentence-transformers`

**Funkcje**:
- `embed(texts: list[str]) -> np.ndarray`: Generowanie embeddings
- Batch processing dla efektywno≈õci

**Model Selection**:
- Default: `all-MiniLM-L6-v2` (384 dim, fast, English)
- Multilingual: `paraphrase-multilingual-MiniLM-L12-v2` (384 dim, slower)

**Dlaczego sentence-transformers?**
- **State-of-the-art**: Najlepsze modele dla sentence embeddings
- **Easy Integration**: Prosty API
- **Model Hub**: Dostƒôp do wielu pre-trained models

#### `index.py` - FAISS Vector Index

**Klasa**: `VectorIndex`

**Backend**: `faiss-cpu`

**Typ Indexu**: `IndexFlatL2` (L2 distance)

**Funkcje**:
- `add_vectors(embeddings)`: Dodawanie wektor√≥w do indexu
- `find_duplicates(threshold) -> list[list[int]]`: Znajdowanie duplikat√≥w

**Algorytm Duplicate Detection**:
1. Dla ka≈ºdego wektora: znajd≈∫ najbli≈ºszych sƒÖsiad√≥w (L2 distance)
2. Je≈õli distance < threshold: to duplikat
3. Grupowanie: connected components (je≈õli A podobny do B i B podobny do C, to A, B, C sƒÖ w jednej grupie)

**Dlaczego FAISS?**
- **Speed**: Zoptymalizowany do vector search
- **Memory**: Kompaktowa reprezentacja
- **Scalability**: Obs≈Çuguje miliony wektor√≥w

### 5.6 Validation Module (`validation/`)

#### `validator.py` - Data Quality Validation

**Klasa**: `DataValidator`

**Funkcje**:
- `validate_data(df, text_column, min_text_length) -> ValidationResult`
- Sprawdzanie `min_length`
- Usuwanie empty/null texts

**Result Object**:
- `success: bool`
- `df: Optional[DataFrame]`
- `error: Optional[str]`
- `report: Optional[dict]`

---

## 6. Hybrid Deduplication Engine

### 6.1 Dlaczego Hybrid?

**Problem**: Semantyczna deduplication jest kosztowna (embeddings + vector search).

**RozwiƒÖzanie**: Dwuetapowa strategia:
1. **Stage 1 (Fast)**: Usu≈Ñ dok≈Çadne duplikaty (hash-based, ~5000 rows/sec)
2. **Stage 2 (Smart)**: Usu≈Ñ semantyczne duplikaty (AI-based, tylko na survivors)

**Korzy≈õci**:
- **Performance**: Stage 1 usuwa 50-80% duplikat√≥w szybko
- **Cost Savings**: Mniej embeddings do wygenerowania
- **Accuracy**: Stage 2 znajduje duplikaty, kt√≥re Stage 1 przegapi≈Ç

### 6.2 Stage 1: Exact Deduplication

**Algorytm**:
1. Normalizacja tekstu (lowercase + whitespace)
2. Hash calculation (xxhash lub MD5)
3. Grupowanie po hash
4. Zostaw pierwszy, usu≈Ñ resztƒô

**Performance**:
- ~5000-6000 rows/second
- Memory: O(n) gdzie n = unikalne hashe
- CPU-only, zero GPU

**Przyk≈Çad**:
```
Text 1: "Hello World"
Text 2: "hello world"  (lowercase)
Text 3: "Hello  World"  (extra spaces)

Po normalizacji: wszystkie ‚Üí "hello world"
Hash: wszystkie ‚Üí same hash
Result: Zostaje tylko Text 1
```

### 6.3 Stage 2: Semantic Deduplication

**Algorytm**:
1. Batch processing (domy≈õlnie 10,000 rows/batch)
2. Embedding generation (sentence-transformers)
3. FAISS index (L2 distance)
4. Duplicate detection (threshold-based)
5. Filtering (zostaw pierwszy w ka≈ºdej grupie)

**Performance**:
- ~500-1000 rows/second (zale≈ºy od modelu)
- Memory: O(n * d) gdzie n = liczba wektor√≥w, d = dimension (384)
- CPU-only (mo≈ºe u≈ºyƒá GPU je≈õli dostƒôpne)

**Przyk≈Çad**:
```
Text 1: "What is the weather today?"
Text 2: "How's the weather?"
Text 3: "Tell me about today's weather"

Wszystkie majƒÖ podobne znaczenie
Embeddings: Wszystkie blisko siebie w przestrzeni wektorowej
Result: Zostaje tylko Text 1
```

### 6.4 Threshold Configuration

**Default**: `--dedup-threshold 0.95` (95% similarity)

**Interpretacja**:
- `0.95`: Bardzo podobne (stricter, mniej duplikat√≥w)
- `0.85`: Umiarkowanie podobne (looser, wiƒôcej duplikat√≥w)
- `0.99`: Prawie identyczne (very strict)

**Konwersja**:
- Cosine similarity ‚Üí L2 distance: `distance = sqrt(2 * (1 - similarity))`
- Przyk≈Çad: similarity=0.95 ‚Üí distance ‚âà 0.316

---

## 7. Memory Management i Optymalizacje

### 7.1 Lazy Evaluation Strategy

**Polars LazyFrame**:
- **Query Planning**: Polars planuje zapytania przed wykonaniem
- **Optimization**: Automatyczne optymalizacje (predicate pushdown, projection)
- **Materialization**: Dane materializowane tylko gdy potrzebne

**Kiedy Materializujemy?**
1. **Chunking**: Wymaga Python functions (regex)
2. **Stage 1 Deduplication**: Hash calculation wymaga warto≈õci
3. **Stage 2 Deduplication**: Embeddings wymagajƒÖ warto≈õci

**Kiedy NIE Materializujemy?**
1. **Schema Validation**: Tylko metadane (`lf.schema`)
2. **Lazy Sanitization**: Operacje na LazyFrame (lowercase, strip)
3. **Filtering**: Polars mo≈ºe filtrowaƒá lazy

### 7.2 Batched Embeddings

**Problem**: Embedding ca≈Çego datasetu naraz ‚Üí OOM dla du≈ºych plik√≥w.

**RozwiƒÖzanie**: Batch processing.

**Algorytm**:
```python
batch_size = 10000  # Configurable via --batch-size

for batch_idx in range(0, len(texts), batch_size):
    batch_texts = texts[batch_idx:batch_idx + batch_size]
    batch_embeddings = embedder.embed(batch_texts)
    index.add_vectors(batch_embeddings)
    # Release batch_texts from memory
    del batch_texts
```

**Korzy≈õci**:
- **Memory Safety**: Tylko jeden batch w pamiƒôci naraz
- **Scalability**: Mo≈ºemy przetwarzaƒá miliony wierszy
- **Configurable**: `--batch-size` dla r√≥≈ºnych system√≥w

**Rekomendacje**:
- **Low-memory (4-8GB)**: `--batch-size 1000`
- **Medium (16GB)**: `--batch-size 10000` (default)
- **High-memory (32GB+)**: `--batch-size 50000`

### 7.3 FAISS Index Management

**Strategia**: Keep index in memory, nie release chunks.

**Dlaczego?**
- FAISS index jest kompaktowy (tylko wektory, nie teksty)
- Potrzebujemy ca≈Çego indexu do duplicate detection
- Release chunks przed detection ‚Üí b≈ÇƒÖd (brak wektor√≥w)

**Memory Usage**:
- FAISS index: ~n * d * 4 bytes (float32)
- Przyk≈Çad: 1M wektor√≥w √ó 384 dim √ó 4 bytes = ~1.5 GB

**Optimization**:
- U≈ºywamy `IndexFlatL2` (najprostszy, najszybszy)
- Mo≈ºna u≈ºyƒá `IndexIVFFlat` dla wiƒôkszych dataset√≥w (future)

### 7.4 STDIN Handling

**Problem**: Polars nie mo≈ºe czytaƒá bezpo≈õrednio z `sys.stdin`.

**RozwiƒÖzanie**: Temporary file.

**Algorytm**:
1. Stream stdin do temporary file (chunked, 64KB chunks)
2. U≈ºyj `pl.scan_ndjson(temp_file)` dla lazy loading
3. Cleanup temp file po zako≈Ñczeniu

**Dlaczego Chunked?**
- Unikamy za≈Çadowania ca≈Çego stdin do RAM
- 64KB chunks sƒÖ efektywne dla I/O

**Implementacja**:
```python
def read_stdin_as_tempfile() -> str:
    CHUNK_SIZE = 64 * 1024  # 64KB
    
    with tempfile.NamedTemporaryFile(mode='wb', delete=False) as tmp:
        while True:
            chunk = sys.stdin.buffer.read(CHUNK_SIZE)
            if not chunk:
                break
            tmp.write(chunk)
        return tmp.name
```

### 7.5 Memory Profiling

**Narzƒôdzie**: `MemoryProfiler` class

**Backend Options**:
- **psutil** (preferred): Dok≈Çadne monitorowanie procesu
- **tracemalloc** (fallback): Built-in Python

**Snapshots**:
- Wykonywane na ka≈ºdym etapie pipeline
- Tracking: peak memory, average memory, growth

**U≈ºycie**:
```bash
entropyguard --input data.jsonl --output clean.jsonl \
  --profile-memory --memory-report-path report.json
```

**Raport**:
```json
{
  "initial_memory_mb": 125.30,
  "snapshots": [
    {"stage": "after_load", "memory_mb": 145.20},
    {"stage": "after_semantic_dedup", "memory_mb": 892.15}
  ],
  "summary": {
    "peak_memory_mb": 892.15,
    "memory_growth_mb": 766.85
  }
}
```

---

## 8. CLI Interface

### 8.1 Argument Parsing (`cli/main.py`)

**Framework**: `argparse`

**Struktura**:
- Standard flags (`--version`, `--verbose`, `--help`)
- Input/Output (`--input`, `--output`)
- Processing options (`--text-column`, `--min-length`, `--dedup-threshold`)
- Advanced options (`--batch-size`, `--profile-memory`, `--config`)

### 8.2 Signal Handling

**Obs≈Çugiwane Signale**:
- `SIGINT` (Ctrl+C): Graceful exit z kodem 130
- `SIGTERM` (Docker/K8s): Graceful exit

**Implementacja**:
```python
def signal_handler(sig, frame):
    print("\n‚ö†Ô∏è  Process interrupted by user. Exiting...", file=sys.stderr)
    cleanup_temp_files()
    sys.exit(130)
```

**Dlaczego Graceful?**
- Cleanup temporary files
- User-friendly message
- Standard exit code (130 = SIGINT)

### 8.3 Error Handling

**Structured Errors**:
- `ValidationError` (code=2): Input validation issues
- `ResourceError` (code=3): OOM, disk space, IO
- `ProcessingError` (code=1): Embedding, FAISS failures

**Error Messages**:
- User-friendly: "‚ùå Error: ..." zamiast traceback
- Hints: Opcjonalne wskaz√≥wki jak naprawiƒá
- Verbose mode: `--verbose` pokazuje pe≈Çny traceback

**JSON Output**:
- `--json` flag: Machine-readable error output
- Format: `{"success": false, "error": "...", "error_code": 2}`

### 8.4 Progress Bars

**Framework**: `tqdm`

**Etapy z Progress Bars**:
- Stage 1: Exact deduplication (rows)
- Stage 2: Semantic deduplication (batches)

**Redirect**:
- Wszystkie progress bars na `stderr`
- `stdout` pozostaje czysty dla JSONL output

**Quiet Mode**:
- `--quiet` flag: Wy≈ÇƒÖcza progress bars
- Przydatne dla non-interactive environments (CI/CD)

### 8.5 Output Modes

**Human-Readable** (default):
- Summary table na `stderr`
- Metrics: rows, duplicates, tokens saved, storage saved

**JSON Output** (`--json`):
- Machine-readable format
- Przydatne dla CI/CD i automatyzacji
- Format: `{"success": true, "stats": {...}, "output_path": "..."}`

**Stdout Mode** (`--output -`):
- Wszystkie logi na `stderr`
- Tylko JSONL data na `stdout`
- Idealne dla Unix pipes

---

## 9. Error Handling

### 9.1 Exception Hierarchy

```
PipelineError (base)
‚îú‚îÄ‚îÄ ValidationError (code=2)
‚îÇ   ‚îî‚îÄ‚îÄ Input validation issues
‚îÇ       - Missing required columns
‚îÇ       - Empty dataset
‚îÇ       - Invalid schema
‚îú‚îÄ‚îÄ ResourceError (code=3)
‚îÇ   ‚îî‚îÄ‚îÄ Resource issues
‚îÇ       - Out of Memory (OOM)
‚îÇ       - Disk space
‚îÇ       - IO errors
‚îî‚îÄ‚îÄ ProcessingError (code=1)
    ‚îî‚îÄ‚îÄ Processing failures
        - Embedding generation errors
        - FAISS index errors
        - Unexpected exceptions
```

### 9.2 Error Codes

| Code | Error Type | Description | When Raised |
|------|------------|-------------|-------------|
| 0 | Success | Pipeline completed | Normal completion |
| 1 | Processing Error | Embedding/FAISS failures | Stage 2 errors |
| 2 | Validation Error | Input validation issues | Schema, empty data |
| 3 | Resource Error | OOM, disk, IO | Memory, file system |
| 130 | SIGINT | User interrupt | Ctrl+C |

### 9.3 Error Messages

**Format**:
```
‚ùå [Error Type]: [Message]
   Hint: [Optional hint]
```

**Przyk≈Çady**:
```
‚ùå Validation Error: Missing required columns: 'text'
   Hint: Available columns: 'id', 'content', 'metadata'

‚ùå Resource Error: Out of memory during embedding generation
   Hint: Try reducing --batch-size or use a smaller dataset
```

### 9.4 JSON Error Output

**Format**:
```json
{
  "success": false,
  "error": "Missing required columns: 'text'",
  "error_code": 2,
  "error_category": "validation",
  "hint": "Available columns: 'id', 'content'"
}
```

**U≈ºycie**:
- CI/CD pipelines
- Automation scripts
- Error monitoring systems

---

## 10. Configuration Management

### 10.1 Config File Support

**Formaty**:
- JSON (`.entropyguardrc.json`)
- YAML (`.entropyguardrc.yaml` / `.entropyguardrc.yml`)
- TOML (`.entropyguardrc.toml`)

**Auto-Detection**:
- Current directory (`.entropyguardrc.*`)
- Home directory (`~/.entropyguardrc.*`)

**Explicit Path**:
- `--config /path/to/config.json`

### 10.2 Config File Format

**Example `.entropyguardrc.json`**:
```json
{
  "text_column": "text",
  "min_length": 50,
  "dedup_threshold": 0.95,
  "model_name": "all-MiniLM-L6-v2",
  "batch_size": 10000,
  "chunk_size": 512,
  "chunk_overlap": 50,
  "show_progress": true
}
```

### 10.3 Priority Order

1. **CLI Arguments** (highest priority)
2. **Config File**
3. **Defaults**

**Przyk≈Çad**:
```bash
# Config file: dedup_threshold=0.95
# CLI: --dedup-threshold 0.85
# Result: 0.85 (CLI overrides config)
```

### 10.4 Config Merging

**Algorytm**:
1. Load config file (if exists)
2. Parse CLI arguments
3. Merge: CLI args override config file values
4. Apply defaults for missing values

**Implementation**: `core/config_loader.py`

---

## 11. Performance i Benchmarking

### 11.1 Performance Metrics

**Measured**:
- **Rows/second**: Processing speed
- **Peak Memory**: Maximum memory usage
- **Memory per 1M rows**: Memory efficiency
- **Total Time**: End-to-end processing time

### 11.2 Benchmark Tool

**Location**: `scripts/benchmark.py`

**Usage**:
```bash
# Single size
python scripts/benchmark.py --size 10K

# Multiple sizes
python scripts/benchmark.py --sizes 1K 10K 100K

# Save results
python scripts/benchmark.py --size 10K --output results.csv
```

**Output Formats**:
- CSV (default)
- JSON (`--format json`)

### 11.3 Typical Performance

**Stage 1 (Exact Deduplication)**:
- ~5000-6000 rows/second
- Memory: O(n) gdzie n = unikalne hashe

**Stage 2 (Semantic Deduplication)**:
- ~500-1000 rows/second (zale≈ºy od modelu)
- Memory: O(n * d) gdzie n = wektory, d = dimension (384)

**Overall**:
- 1K rows: ~2-3 seconds
- 10K rows: ~15-20 seconds
- 100K rows: ~2-3 minutes

### 11.4 Memory Efficiency

**Per 1M rows**:
- Small datasets (<10K): ~650 MB per 1M rows
- Medium datasets (10K-100K): ~560 MB per 1M rows
- Large datasets (>100K): ~500 MB per 1M rows

**Optimization Tips**:
- Reduce `--batch-size` for low-memory systems
- Use `--dry-run` for testing (skips expensive operations)
- Enable `--profile-memory` to identify bottlenecks

---

## 12. Testing Strategy

### 12.1 Test Coverage

**Current**: ~74% overall coverage

**Module Coverage**:
- `core/types.py`: 100%
- `core/errors.py`: 62%
- `core/memory_profiler.py`: 77%
- `ingestion/loader.py`: 90%
- `cli/main.py`: 51%

### 12.2 Test Types

**Unit Tests**:
- Individual functions
- Edge cases
- Error handling

**Integration Tests**:
- Full pipeline execution
- End-to-end workflows
- CLI integration

**Performance Tests**:
- Benchmark script
- Memory profiling
- Scalability tests

### 12.3 Test Files

```
tests/
‚îú‚îÄ‚îÄ test_core_errors.py          # Exception hierarchy
‚îú‚îÄ‚îÄ test_core_types.py           # Type definitions
‚îú‚îÄ‚îÄ test_core_pipeline_v1_20.py # Pipeline integration
‚îú‚îÄ‚îÄ test_core_sanitization_lazy.py # Lazy sanitization
‚îú‚îÄ‚îÄ test_cli_main.py             # CLI functions
‚îú‚îÄ‚îÄ test_cli_integration.py      # CLI end-to-end
‚îú‚îÄ‚îÄ test_ingestion_loader.py     # Data loading
‚îú‚îÄ‚îÄ test_memory_profiler.py      # Memory profiling
‚îî‚îÄ‚îÄ ...
```

### 12.4 Running Tests

```bash
# All tests
pytest tests/ -v

# With coverage
pytest tests/ --cov=src/entropyguard --cov-report=html

# Specific test file
pytest tests/test_core_pipeline_v1_20.py -v
```

---

## 13. Deployment i Distribution

### 13.1 Installation Methods

**Developer Workflow**:
```bash
git clone https://github.com/DamianSiuta/entropyguard.git
cd entropyguard
python -m poetry install
```

**End User (pip + Git)**:
```bash
pip install "git+https://github.com/DamianSiuta/entropyguard.git"
```

**Docker**:
```bash
docker build -t entropyguard .
docker run -v $(pwd)/data:/data entropyguard \
  --input /data/file.jsonl --output /data/clean.jsonl
```

### 13.2 Dependencies

**Core Dependencies**:
- `polars ^0.20.0`: DataFrame library
- `sentence-transformers ^2.0.0`: Embeddings
- `faiss-cpu ^1.7.4`: Vector search
- `xxhash ^3.4.0`: Fast hashing
- `tqdm ^4.66.0`: Progress bars

**Optional Dependencies**:
- `psutil`: Better memory profiling
- `PyYAML`: YAML config support
- `tomli`: TOML config support (Python <3.11)

### 13.3 Python Version Support

**Supported**: Python 3.10, 3.11, 3.12

**Not Supported**: Python 3.13 (missing wheels for `numpy < 2.0`, `faiss-cpu`)

### 13.4 Docker Image

**Base**: `python:3.10-slim`

**Size**: ~1.5 GB (includes PyTorch, sentence-transformers, FAISS)

**Optimization**:
- Multi-stage build (future)
- Layer caching
- Minimal dependencies

---

## 14. Roadmap i Wersjonowanie

### 14.1 Version History

**v1.11** (Proof of Concept):
- Basic hybrid deduplication
- Unix pipes support
- CLI interface

**v1.20** (Production-Grade):
- Structured error handling
- Memory profiling
- JSON output
- Config file support
- Progress bars
- Batched embeddings

**v1.21** (Current Development):
- Performance benchmarks
- Memory profiling tools
- Documentation updates
- Test coverage improvements

### 14.2 Future Plans

**v1.22+**:
- Streaming FAISS (dla 100GB+ datasets)
- GPU support (optional)
- More embedding models
- Performance optimizations

**Enterprise Features** (Separate):
- Web Dashboard
- RESTful API
- Real-time Monitoring
- Alert System
- SSO Integration

### 14.3 Open Core Strategy

**Community Edition** (Open Source):
- CLI tool
- Core pipeline
- All data processing features
- MIT License

**Enterprise Edition** (Commercial):
- Everything in Community
- Web Dashboard
- API
- Monitoring & Alerts
- SSO
- Commercial License

---

## 15. Podsumowanie

### 15.1 Kluczowe Cechy

1. **Hybrid Deduplication**: Dwuetapowa strategia (hash + AI)
2. **Lazy Evaluation**: Polars LazyFrame dla memory efficiency
3. **Batched Processing**: Scalable embedding generation
4. **Structured Errors**: Production-grade error handling
5. **Memory Profiling**: Debugging OOM issues
6. **Config Files**: Convenience dla teams
7. **Unix Pipes**: Integration z existing workflows
8. **Type Safety**: Full type hints, TypedDict, dataclasses

### 15.2 Design Decisions

**Dlaczego Polars?**
- Lazy evaluation
- High performance
- Memory efficiency
- Modern API

**Dlaczego FAISS?**
- Fast vector search
- Scalable
- Industry standard

**Dlaczego sentence-transformers?**
- State-of-the-art models
- Easy integration
- Model hub access

**Dlaczego Hybrid Deduplication?**
- Performance: Stage 1 usuwa 50-80% szybko
- Accuracy: Stage 2 znajduje semantyczne duplikaty
- Cost: Mniej embeddings = mniej koszt√≥w

### 15.3 Use Cases

1. **LLM Training Data Preparation**: Clean datasets przed treningiem
2. **RAG Pipeline**: Chunking + deduplication dla retrieval
3. **Data Quality**: Validation i sanitization
4. **Compliance**: Audit logs dla GDPR/CCPA
5. **Cost Optimization**: Redukcja duplikat√≥w = mniej API calls

---

**Koniec Dokumentacji**

*Ostatnia aktualizacja: v1.20.0*


