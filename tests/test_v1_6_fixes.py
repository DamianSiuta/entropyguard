"""
Tests for v1.6.0 critical bugfixes.

This test suite verifies that the critical issues identified in the technical audit
have been properly fixed:
1. FAISS mathematical formula correction
2. Recursion depth limit in chunking
3. Zero vector edge case handling
"""

import warnings
from unittest.mock import patch, MagicMock

import numpy as np
import polars as pl
import pytest

from entropyguard.chunking import Chunker
from entropyguard.deduplication import Embedder, VectorIndex
from entropyguard.cli.pipeline import Pipeline


class TestFAISSMathFix:
    """Test that FAISS distance threshold calculation is mathematically correct."""

    def test_threshold_calculation_no_square_root(self) -> None:
        """
        Verify that threshold is calculated as squared distance, not distance.

        For normalized vectors: dÂ² = 2(1 - cosine_similarity)
        For dedup_threshold=0.90: threshold_squared = 2 * (1 - 0.90) = 0.2
        NOT sqrt(2 * (1 - 0.90)) = 0.447
        """
        dedup_threshold = 0.90
        expected_threshold_squared = 2.0 * (1.0 - dedup_threshold)  # Should be 0.2
        wrong_threshold = (2.0 * (1.0 - dedup_threshold)) ** 0.5  # Wrong: 0.447

        # Verify the correct formula
        assert expected_threshold_squared == pytest.approx(0.2, abs=1e-6)
        assert wrong_threshold == pytest.approx(0.447, abs=1e-3)
        assert expected_threshold_squared != pytest.approx(wrong_threshold, abs=1e-2)

    def test_pipeline_uses_correct_threshold_formula(self) -> None:
        """
        Test that Pipeline calculates threshold correctly by checking the behavior.

        We can't directly access the threshold in Pipeline, but we can verify
        that the deduplication behaves correctly with known similarity values.
        """
        # Create test data with known similarity
        # Two texts that should be similar (but not identical)
        texts = [
            "What is the balance of my account?",
            "Can you tell me my account balance?",
            "Completely different unrelated text about something else entirely.",
        ]

        # Create embeddings
        embedder = Embedder()
        embeddings = embedder.embed(texts)

        # Verify embeddings are normalized (L2 norm should be ~1.0)
        for i, emb in enumerate(embeddings):
            norm = np.linalg.norm(emb)
            assert norm == pytest.approx(1.0, abs=0.01), f"Embedding {i} not normalized: norm={norm}"

        # Create index and add vectors
        index = VectorIndex(dimension=384)
        index.add_vectors(embeddings)

        # Test with threshold for 0.90 similarity
        # threshold_squared = 2 * (1 - 0.90) = 0.2
        threshold_squared_090 = 2.0 * (1.0 - 0.90)
        duplicate_groups_090 = index.find_duplicates(threshold=threshold_squared_090)

        # Test with threshold for 0.95 similarity (stricter)
        # threshold_squared = 2 * (1 - 0.95) = 0.1
        threshold_squared_095 = 2.0 * (1.0 - 0.95)
        duplicate_groups_095 = index.find_duplicates(threshold=threshold_squared_095)

        # With 0.90 threshold, first two texts should be duplicates
        # With 0.95 threshold (stricter), they might not be
        # This verifies that the threshold is being used correctly

        # Verify that threshold_squared_090 is 0.2 (not 0.447)
        assert threshold_squared_090 == pytest.approx(0.2, abs=1e-6)
        assert threshold_squared_095 == pytest.approx(0.1, abs=1e-6)

    def test_faiss_returns_squared_distances(self) -> None:
        """
        Verify that FAISS IndexFlatL2 returns squared distances, not distances.

        This is critical for the threshold calculation to work correctly.
        """
        # Create two normalized vectors
        vec1 = np.random.randn(384).astype(np.float32)
        vec1 = vec1 / np.linalg.norm(vec1)  # Normalize

        vec2 = np.random.randn(384).astype(np.float32)
        vec2 = vec2 / np.linalg.norm(vec2)  # Normalize

        # Calculate expected squared distance manually
        diff = vec1 - vec2
        expected_squared_distance = np.dot(diff, diff)

        # Add to FAISS index
        index = VectorIndex(dimension=384)
        index.add_vectors(vec1.reshape(1, -1))
        index.add_vectors(vec2.reshape(1, -1))

        # Search for nearest neighbor
        distances, indices = index.search(vec1.reshape(1, -1), k=2)

        # FAISS should return squared distances
        # The distance to vec1 itself should be ~0, distance to vec2 should match expected
        assert distances[0][0] == pytest.approx(0.0, abs=1e-5)  # Distance to itself
        assert distances[0][1] == pytest.approx(expected_squared_distance, abs=1e-4)


class TestChunkerRecursionLimit:
    """Test that chunker has recursion depth limit to prevent stack overflow."""

    def test_recursion_limit_on_long_text_without_separators(self) -> None:
        """
        Test that chunker handles very long text without separators without hanging.

        This tests the recursion limit fix by providing pathological input.
        """
        # Create malicious input: 1000 characters of 'A' with no separators
        malicious_text = "A" * 1000

        chunker = Chunker(chunk_size=10, chunk_overlap=2)

        # This should NOT raise RecursionError or hang
        # It should use hard split fallback
        chunks = chunker.split_text(malicious_text)

        # Verify it returns chunks
        assert len(chunks) > 0
        assert all(isinstance(chunk, str) for chunk in chunks)
        assert all(len(chunk) <= 10 for chunk in chunks)

        # Verify all characters are preserved
        reconstructed = "".join(chunks)
        assert len(reconstructed) == len(malicious_text)
        assert reconstructed == malicious_text

    def test_recursion_limit_with_nested_separators(self) -> None:
        """
        Test recursion limit with nested/recursive separator structure.

        This simulates a case where recursion could go deep.
        """
        # Create text that requires deep recursion
        # Use a separator that appears many times
        text = "|".join(["A" * 50] * 20)  # 20 segments, each 50 chars

        chunker = Chunker(chunk_size=10, chunk_overlap=2, separators=["|", " ", ""])

        # Should not raise RecursionError
        chunks = chunker.split_text(text)

        assert len(chunks) > 0
        assert all(len(chunk) <= 10 for chunk in chunks)

    def test_recursion_limit_explicit_depth_check(self) -> None:
        """
        Test that _split_recursively respects max depth limit.

        We'll use a mock or direct call to verify depth checking.
        """
        chunker = Chunker(chunk_size=10, chunk_overlap=2)

        # Create text that would require >100 levels of recursion if not limited
        # Use a pattern that creates deep nesting
        text = "A" * 2000  # Very long text without separators

        # This should trigger hard split after max depth, not recurse infinitely
        chunks = chunker.split_text(text)

        # Should return chunks (hard split fallback)
        assert len(chunks) > 0
        assert all(len(chunk) <= 10 for chunk in chunks)

        # Verify no data loss
        reconstructed = "".join(chunks)
        assert len(reconstructed) == len(text)


class TestZeroVectorHandling:
    """Test that zero vectors are handled gracefully."""

    def test_zero_vector_filtered_before_adding_to_index(self) -> None:
        """
        Test that zero vectors are filtered out and don't cause FAISS errors.

        Zero vectors would cause false positives in duplicate detection.
        """
        index = VectorIndex(dimension=384)

        # Create a mix of normal and zero vectors
        normal_vector = np.random.randn(384).astype(np.float32)
        normal_vector = normal_vector / np.linalg.norm(normal_vector)  # Normalize

        zero_vector = np.zeros(384, dtype=np.float32)

        # Create array with both
        vectors = np.vstack([normal_vector, zero_vector, normal_vector])

        # Add vectors - should filter out zero vector and warn
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
            index.add_vectors(vectors)

            # Should have warning about zero vector
            assert len(w) > 0
            assert any("zero vector" in str(warning.message).lower() for warning in w)

        # Index should only have 2 vectors (the two normal ones)
        assert index.size() == 2

    def test_all_zero_vectors_handled_gracefully(self) -> None:
        """
        Test that if all vectors are zero, index remains empty without error.
        """
        index = VectorIndex(dimension=384)

        # Create array of all zero vectors
        zero_vectors = np.zeros((3, 384), dtype=np.float32)

        # Should not raise error, just skip all vectors
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
            index.add_vectors(zero_vectors)

            # Should have warnings
            assert len(w) >= 1

        # Index should be empty
        assert index.size() == 0

    def test_zero_vector_near_zero_norm_handled(self) -> None:
        """
        Test that vectors with very small (but non-zero) norm are handled.

        Epsilon threshold should catch near-zero vectors.
        """
        index = VectorIndex(dimension=384)

        # Create vector with very small norm (but not exactly zero)
        tiny_vector = np.ones(384, dtype=np.float32) * 1e-10  # Very small values
        # Norm will be sqrt(384 * 1e-20) = ~1.96e-9, squared = ~3.84e-18 < 1e-8

        vectors = tiny_vector.reshape(1, -1)

        # Should be filtered out (norm squared < epsilon)
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
            index.add_vectors(vectors)

            # Should warn about zero vector
            assert len(w) > 0

        # Index should be empty
        assert index.size() == 0

    def test_normal_vectors_not_filtered(self) -> None:
        """
        Test that normal (non-zero) vectors are NOT filtered out.

        This ensures the zero-vector filtering doesn't break normal operation.
        """
        index = VectorIndex(dimension=384)

        # Create normal normalized vectors
        vec1 = np.random.randn(384).astype(np.float32)
        vec1 = vec1 / np.linalg.norm(vec1)

        vec2 = np.random.randn(384).astype(np.float32)
        vec2 = vec2 / np.linalg.norm(vec2)

        vectors = np.vstack([vec1, vec2])

        # Should add without warnings (no zero vectors)
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")
            index.add_vectors(vectors)

            # Should NOT have zero vector warnings
            zero_warnings = [warn for warn in w if "zero vector" in str(warn.message).lower()]
            assert len(zero_warnings) == 0

        # Index should have both vectors
        assert index.size() == 2


class TestIntegrationFixes:
    """Integration tests combining multiple fixes."""

    def test_pipeline_with_all_fixes(self) -> None:
        """
        Integration test: Run pipeline with settings that would trigger all fixes.

        - Uses dedup_threshold=0.90 (tests FAISS math)
        - Includes long text without separators (tests recursion limit)
        - Includes empty text that becomes zero vector (tests zero vector handling)
        """
        # Create test data
        test_data = pl.DataFrame({
            "text": [
                "This is a normal text that should be processed correctly.",
                "A" * 500,  # Long text without separators (tests recursion)
                "",  # Empty text (will become zero vector after sanitization)
                "Another normal text for testing purposes.",
                "A" * 300,  # Another long text
            ],
            "id": [0, 1, 2, 3, 4],
        })

        # Save to temporary file
        import tempfile
        import os

        with tempfile.NamedTemporaryFile(mode="w", suffix=".jsonl", delete=False) as f:
            input_path = f.name
            test_data.write_ndjson(input_path)

        try:
            with tempfile.NamedTemporaryFile(mode="w", suffix=".jsonl", delete=False) as f:
                output_path = f.name

            try:
                # Run pipeline with dedup_threshold=0.90 (tests FAISS math fix)
                pipeline = Pipeline()
                result = pipeline.run(
                    input_path=input_path,
                    output_path=output_path,
                    text_column="text",
                    min_length=10,
                    dedup_threshold=0.90,  # Tests threshold calculation
                )

                # Pipeline should complete without errors
                # (empty text will be filtered, long texts will be chunked safely)
                assert result["success"] is True

                # Verify output exists
                assert os.path.exists(output_path)

                # Read output
                output_df = pl.read_ndjson(output_path)

                # Should have some rows (empty text filtered, long texts processed)
                assert output_df.height >= 0

            finally:
                if os.path.exists(output_path):
                    os.unlink(output_path)

        finally:
            if os.path.exists(input_path):
                os.unlink(input_path)

